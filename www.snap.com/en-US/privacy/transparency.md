Transparency Report
July 1, 2023 – December 31, 2023

Released:

April 25, 2024

Updated:

April 25, 2024

To further provide insight into Snap’s safety efforts and the nature and volume of content reported on our platform, we publish this transparency report twice a year. We are committed to continuing to make these reports more comprehensive and informative to the safety and well-being of our community, and the many stakeholders who care deeply about our content moderation and law enforcement practices. 

This Transparency Report covers the second half of 2023 (July 1 - December 31). As with our previous reports, we share data about the global number of in-app content and account-level reports we received and enforced across specific categories of policy violations; how we responded to requests from law enforcement and governments; and our enforcement actions broken down by country.

As part of our ongoing commitment to continually improve our transparency reports, we are introducing a few new elements with this release. 

First, we have expanded our main table to include reports and enforcement against content and accounts tied to both Terrorism & Violent Extremism and Child Sexual Exploitation & Abuse (CSEA). In previous reports, we had highlighted account deletions made in response to those violations in separate sections. We will continue to outline our proactive and reactive efforts against CSEA, as well as our reports to NCMEC, in a separate section. 

Second, we have provided expanded information on appeals, outlining total appeals and reinstatements by Community Guidelines enforcements. 

Finally, we have expanded our European Union section, providing increased insight into Snap’s EU activities. Specifically, we are publishing our most recent DSA Transparency Report and additional metrics regarding our CSEA media scanning.

For more information about our policies for combating online harms, and plans to continue evolving our reporting practices, please read our recent Safety & Impact blog about this Transparency Report. To find additional safety and privacy resources on Snapchat, see our About Transparency Reporting tab at the bottom of the page.


Please note that the most up-to-date version of this Transparency Report can be found in the en-US locale.

Overview of Content and Account Violations

From July 1 - December 31, 2023, Snap enforced against 5,376,714 pieces of content globally that were reported to us and violated our Community Guidelines.

During the reporting period, we saw a Violative View Rate (VVR) of 0.01 percent, which means that out of every 10,000 Snap and Story views on Snapchat, 1 contained content found to violate our policies. The median turnaround time to enforce reported content was ~10 minutes.

Total Content & Account Reports	Total Content Enforced	Total Unique Accounts Enforced
19,041,510	5,376,714	3,315,759
Reason	Content & Account Reports	Content Enforced	% of the Total Content Enforced by Snap	Unique Accounts Enforced	Turnaround Time (in median minutes)
Sexual Content	4,271,116	2,266,213	42.1%	1,321,205	7
Child Sexual Exploitation	847,430	239,820	4.5%	206,904	52
Harassment and Bullying	8,524,054	1,193,695	22.2%	934,994	7
Threats & Violence	836,125	114,315	2.1%	83,743	27
Self-Harm & Suicide	188,124	32,841	0.6%	28,207	44
False Information	439,233	1,463	0.1%	1,277	12
Impersonation	440,437	14,557	0.3%	14,381	3
Spam	1,981,115	1,002,278	18.6%	645,238	2
Drugs	368,732	241,227	4.5%	166,562	55
Weapons	115,512	13,271	0.2%	9,768	35
Other Regulated Goods	478,665	140,554	2.6%	99,768	32
Hate Speech	431,670	113,906	2.1%	97,621	46
Terrorism & Violent Extremism	119,297	2,574	0.1%	2,136	45

Analysis of Content and Account Violations

Our overall reporting and enforcement rates remained fairly similar to the previous six months. This cycle, we saw an approximate 10% increase in total content and account reports.

The Israel-Hamas conflict began during this period, and as a result we saw an uptick in violative content. Total reports related to hate speech increased by ~61%, while total content enforcements of hate speech increased by ~97% and unique account enforcements increased by ~124%. Terrorism & Violent extremism reports and enforcements have also increased, though they comprise <0.1% of the total content enforcements on our platform. Our Trust & Safety teams continue to remain vigilant as global conflicts arise in order to help keep Snapchat safe. We have also expanded our transparency report to include more information at a global and country-level regarding the total reports, content enforced, and unique accounts enforced for violations of our Terrorism & Violent Extremism policy. 

Combating Child Sexual Exploitation & Abuse

Sexual exploitation of any member of our community, especially minors, is illegal, abhorrent, and prohibited by our Community Guidelines. Preventing, detecting, and eradicating Child Sexual Exploitation and Abuse (CSEA) on our platform is a top priority for Snap, and we continually evolve our capabilities to combat these and other crimes.

We use active technology detection tools, such as PhotoDNA robust hash-matching and Google’s Child Sexual Abuse Imagery (CSAI) Match to identify known illegal images and videos of child sexual abuse, respectively, and report them to the U.S. National Center for Missing and Exploited Children (NCMEC), as required by law. NCMEC then, in turn, coordinates with domestic or international law enforcement, as required.

In the second half of 2023, we proactively detected and actioned 59% of the total child sexual exploitation and abuse violations reported. This reflects a 39% total decrease from the previous period due to enhancements in Snapchatters’ options for reporting, increasing our visibility of potential CSEA sent on Snapchat. 

Reason	Total Content Enforced	Total Accounts Disabled	Total Submissions to NCMEC
CSEA	1,046,296	343,865	398,736

*Note that each submission to NCMEC can contain multiple pieces of content. The total individual pieces of media submitted to NCMEC is equal to our total content enforced. We also have excluded retracted submissions to NCMEC from this number.

Self-harm and Suicide Content

We care deeply about the mental health and well-being of Snapchatters, which continues to inform our decisions to build Snapchat differently. As a platform designed for communications between and among real friends, we believe Snapchat can play a unique role in empowering friends to help each other in difficult times.

When our Trust & Safety team becomes aware of a Snapchatter in distress, they can forward self-harm prevention and support resources, and notify emergency response personnel when appropriate. The resources that we share are available on our global list of safety resources, and are publicly available to all Snapchatters.

Total Times Suicide Resources Shared
28,361

Appeals

In our previous report, we introduced metrics on appeals, where we highlighted the number of times users asked us to re-review our initial moderation decision against their account. In this report, we have expanded our appeals to capture the full range of our policy categories for account-level violations.

Policy Reason	Total Appeals	Total Reinstatements	Total Decisions Upheld	Median Turnaround Time (days) to Process Appeals
Total	520,962	20,982	473,475	2
Sexual Content	161,446	2,345	159,101	2
Child Sexual Exploitation*	116,736	12,860	77,371	152
Harassment and Bullying	19,919	535	19,384	2
Threats & Violence	1,667	87	1,580	57
Self-Harm & Suicide	41	7	34	34
False Information	9	0	9	20
Impersonation	1,558	114	1,444	19
Spam	13,745	193	13,552	4
Drugs	192,947	4,487	188,460	0
Weapons	2,936	83	2,853	27
Other Regulated Goods	9,356	208	9,148	13
Hate Speech	484	54	430	63
Terrorism & Violent Extremism	118	9	109	46

* Stopping the spread of content or activity related to child sexual exploitation is a top priority. Snap devotes significant resources toward this goal and has zero tolerance for such conduct.  Special training is required to review CSE appeals, and there is a limited team of agents that handles these reviews due to the graphic nature of the content.  In the fall of 2023, Snap implemented policy changes that affected the consistency of certain CSE enforcements; we have addressed these inconsistencies through agent re-training and quality assurance.  We expect that Snap’s next transparency report will reveal progress toward improving response times for CSE appeals and improving the precision of initial enforcement actions.

Ads Moderation

Snap is committed to ensuring that all ads are fully compliant with our advertising policies. We believe in a responsible and respectful approach to advertising, creating a safe and enjoyable experience for all of our users. Below we have included insight into our moderation for paid advertisements on Snapchat. Note that ads on Snapchat can be removed for a variety of reasons as outlined in Snap’s Advertising Policies, including deceptive content, adult content, violent or disturbing content, hate speech, and intellectual property infringement. Additionally, you can now find Snapchat’s Ads Gallery in the navigation bar of this transparency report.

Total Ads Reported	Total Ads Removed
20,204	7,258

Regional & Country Overview

This section provides an overview of the enforcement of our Community Guidelines in a sampling of geographic regions. Our Guidelines apply to all content on Snapchat—and all Snapchatters—across the globe, regardless of location.

Information for individual countries, including all EU Member States, is available for download via the attached CSV file.

Download CSV
Region	Content & Account Reports	Content Enforced	Unique Accounts Enforced
North America	6,146,896	2,253,147	1,377,547
Europe	5,060,287	1,471,363	963,225
Rest of World	7,834,327	1,652,204	1,011,616
Total	19,041,510	5,376,714	3,315,759
Australia
Austria
Belgium
Brazil
Canada
Denmark
Finland
France
Germany
India
Iraq
Ireland
Italy
Mexico
Netherlands
New Zealand
Norway
Poland
Saudi Arabia
Spain
Sweden
Turkey
United Arab Emirates
United Kingdom
United States
Government & Intellectual Property Removal Requests

Learn More
About Transparency Reporting

Learn More
Glossary of Transparency Report

Learn More